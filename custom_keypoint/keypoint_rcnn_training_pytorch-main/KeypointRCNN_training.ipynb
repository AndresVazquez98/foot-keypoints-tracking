{"cells":[{"cell_type":"markdown","metadata":{"id":"IW81o88IORn5"},"source":["Detailed explanation:\n","https://medium.com/@alexppppp/how-to-train-a-custom-keypoint-detection-model-with-pytorch-d9af90e111da\n","\n","GitHub repo:\n","https://github.com/alexppppp/keypoint_rcnn_training_pytorch"]},{"cell_type":"markdown","metadata":{"id":"NnlGxI8RORn8"},"source":["# 1. Imports"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bW3jkfJvQ9rG","executionInfo":{"status":"ok","timestamp":1651419403455,"user_tz":300,"elapsed":21048,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}},"outputId":"97a6bc6f-5714-4ac1-a059-de8972e5b0aa"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install -U albumentations"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5NDQqOUYEXE","executionInfo":{"status":"ok","timestamp":1651421239845,"user_tz":300,"elapsed":2561,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}},"outputId":"bc7e1fd8-4fc7-4431-e31f-1a6b36331d0b"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (1.1.0)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.21.6)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (4.2.0)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.4.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.8)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/custom_keypoint/keypoint_rcnn_training_pytorch-main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Y0jPYyMRJyD","executionInfo":{"status":"ok","timestamp":1651419430250,"user_tz":300,"elapsed":156,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}},"outputId":"7546996a-0b02-48ef-ed74-853420c6a546"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/custom_keypoint/keypoint_rcnn_training_pytorch-main\n"]}]},{"cell_type":"code","execution_count":32,"metadata":{"id":"xxrQWbmmORn9","executionInfo":{"status":"ok","timestamp":1651421249668,"user_tz":300,"elapsed":1,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}}},"outputs":[],"source":["import os, json, cv2, numpy as np, matplotlib.pyplot as plt\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","import torchvision\n","from torchvision.models.detection.rpn import AnchorGenerator\n","from torchvision.transforms import functional as F\n","\n","import albumentations as A # Library for augmentations"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Ujl8ubo1ORn-","executionInfo":{"status":"ok","timestamp":1651421251240,"user_tz":300,"elapsed":1,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}}},"outputs":[],"source":["# https://github.com/pytorch/vision/tree/main/references/detection\n","import transforms, utils, engine, train\n","from utils import collate_fn\n","from engine import train_one_epoch, evaluate"]},{"cell_type":"markdown","metadata":{"id":"edVmeXfCORn-"},"source":["# 2. Augmentations"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"zS5RgV_eORn-","executionInfo":{"status":"ok","timestamp":1651421253300,"user_tz":300,"elapsed":132,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}}},"outputs":[],"source":["def train_transform():\n","    return A.Compose([\n","            A.RandomRotate90(p=1), # Random rotation of an image by 90 degrees zero or more times\n","            A.RandomBrightnessContrast( p=0.5), # Random change of brightness & contrast\n","      \n","    ],\n","    keypoint_params=A.KeypointParams(format='xy'), # More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\n","    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['bboxes_labels']) # Bboxes should have labels, read more here https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n","    )"]},{"cell_type":"markdown","metadata":{"id":"hdds-QEmORn_"},"source":["# 3. Dataset class"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"tGA1T9fdORn_","executionInfo":{"status":"ok","timestamp":1651421255915,"user_tz":300,"elapsed":163,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}}},"outputs":[],"source":["class ClassDataset(Dataset):\n","    def __init__(self, root, transform=None, demo=False):                \n","        self.root = root\n","        self.transform = transform\n","        self.demo = demo # Use demo=True if you need transformed and original images (for example, for visualization purposes)\n","        self.imgs_files = sorted(os.listdir(os.path.join(root, \"images\")))\n","        self.annotations_files = sorted(os.listdir(os.path.join(root, \"annotations\")))\n","    \n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.root, \"images\", self.imgs_files[idx])\n","        annotations_path = os.path.join(self.root, \"annotations\", self.annotations_files[idx])\n","\n","        img_original = cv2.imread(img_path)\n","        img_original = cv2.cvtColor(img_original, cv2.COLOR_BGR2RGB)        \n","        \n","        with open(annotations_path) as f:\n","            data = json.load(f)\n","            bboxes_original = data['bboxes']\n","            keypoints_original = data['keypoints']\n","            \n","            # All objects are glue tubes\n","            bboxes_labels_original = ['Glue tube' for _ in bboxes_original]            \n","\n","        if self.transform:   \n","            # Converting keypoints from [x,y,visibility]-format to [x, y]-format + Flattening nested list of keypoints            \n","            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n","            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]], where each keypoint is in [x, y]-format            \n","            # Then we need to convert it to the following list:\n","            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2]\n","            keypoints_original_flattened = [el[0:2] for kp in keypoints_original for el in kp]\n","            \n","            # Apply augmentations\n","            transformed = self.transform(image=img_original, bboxes=bboxes_original, bboxes_labels=bboxes_labels_original, keypoints=keypoints_original_flattened)\n","            img = transformed['image']\n","            bboxes = transformed['bboxes']\n","            \n","            # Unflattening list transformed['keypoints']\n","            # For example, if we have the following list of keypoints for three objects (each object has two keypoints):\n","            # [obj1_kp1, obj1_kp2, obj2_kp1, obj2_kp2, obj3_kp1, obj3_kp2], where each keypoint is in [x, y]-format\n","            # Then we need to convert it to the following list:\n","            # [[obj1_kp1, obj1_kp2], [obj2_kp1, obj2_kp2], [obj3_kp1, obj3_kp2]]\n","            keypoints_transformed_unflattened = np.reshape(np.array(transformed['keypoints']), (-1,2,2)).tolist()\n","\n","            # Converting transformed keypoints from [x, y]-format to [x,y,visibility]-format by appending original visibilities to transformed coordinates of keypoints\n","            keypoints = []\n","            for o_idx, obj in enumerate(keypoints_transformed_unflattened): # Iterating over objects\n","                obj_keypoints = []\n","                for k_idx, kp in enumerate(obj): # Iterating over keypoints in each object\n","                    # kp - coordinates of keypoint\n","                    # keypoints_original[o_idx][k_idx][2] - original visibility of keypoint\n","                    obj_keypoints.append(kp + [keypoints_original[o_idx][k_idx][2]])\n","                keypoints.append(obj_keypoints)\n","        \n","        else:\n","            img, bboxes, keypoints = img_original, bboxes_original, keypoints_original        \n","        \n","        # Convert everything into a torch tensor        \n","        bboxes = torch.as_tensor(bboxes, dtype=torch.float32)       \n","        target = {}\n","        target[\"boxes\"] = bboxes\n","        target[\"labels\"] = torch.as_tensor([1 for _ in bboxes], dtype=torch.int64) # all objects are glue tubes\n","        target[\"image_id\"] = torch.tensor([idx])\n","        target[\"area\"] = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])\n","        target[\"iscrowd\"] = torch.zeros(len(bboxes), dtype=torch.int64)\n","        target[\"keypoints\"] = torch.as_tensor(keypoints, dtype=torch.float32)        \n","        img = F.to_tensor(img)\n","        \n","        bboxes_original = torch.as_tensor(bboxes_original, dtype=torch.float32)\n","        target_original = {}\n","        target_original[\"boxes\"] = bboxes_original\n","        target_original[\"labels\"] = torch.as_tensor([1 for _ in bboxes_original], dtype=torch.int64) # all objects are glue tubes\n","        target_original[\"image_id\"] = torch.tensor([idx])\n","        target_original[\"area\"] = (bboxes_original[:, 3] - bboxes_original[:, 1]) * (bboxes_original[:, 2] - bboxes_original[:, 0])\n","        target_original[\"iscrowd\"] = torch.zeros(len(bboxes_original), dtype=torch.int64)\n","        target_original[\"keypoints\"] = torch.as_tensor(keypoints_original, dtype=torch.float32)        \n","        img_original = F.to_tensor(img_original)\n","\n","        if self.demo:\n","            return img, target, img_original, target_original\n","        else:\n","            return img, target\n","    \n","    def __len__(self):\n","        return len(self.imgs_files)"]},{"cell_type":"markdown","metadata":{"id":"whXhEBqtORoA"},"source":["# 4. Visualizing a random item from dataset"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"id":"Nf0huMkEORoB","executionInfo":{"status":"error","timestamp":1651421258245,"user_tz":300,"elapsed":133,"user":{"displayName":"oscar frausto","userId":"04228183479494857501"}},"outputId":"fbf6e0f4-3557-4dd4-c030-774947fbc568"},"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-a2726a244826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mKEYPOINTS_FOLDER_TRAIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'glue_tubes_keypoints_dataset_134imgs/train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKEYPOINTS_FOLDER_TRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-5f21f1fd5800>\u001b[0m in \u001b[0;36mtrain_transform\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     ],\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkeypoint_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeypointParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'xy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# More about keypoint formats used in albumentations library read at https://albumentations.ai/docs/getting_started/keypoints_augmentation/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mbbox_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBboxParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pascal_voc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_fields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bboxes_labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Bboxes should have labels, read more here https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n","\u001b[0;31mAttributeError\u001b[0m: module 'albumentations' has no attribute 'KeypointParams'"]}],"source":["KEYPOINTS_FOLDER_TRAIN = 'glue_tubes_keypoints_dataset_134imgs/train'\n","dataset = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=True)\n","data_loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","\n","iterator = iter(data_loader)\n","batch = next(iterator)\n","\n","print(\"Original targets:\\n\", batch[3], \"\\n\\n\")\n","print(\"Transformed targets:\\n\", batch[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HL4X5ztQORoB"},"outputs":[],"source":["keypoints_classes_ids2names = {0: 'Head', 1: 'Tail'}\n","\n","def visualize(image, bboxes, keypoints, image_original=None, bboxes_original=None, keypoints_original=None):\n","    fontsize = 18\n","\n","    for bbox in bboxes:\n","        start_point = (bbox[0], bbox[1])\n","        end_point = (bbox[2], bbox[3])\n","        image = cv2.rectangle(image.copy(), start_point, end_point, (0,255,0), 2)\n","    \n","    for kps in keypoints:\n","        for idx, kp in enumerate(kps):\n","            image = cv2.circle(image.copy(), tuple(kp), 5, (255,0,0), 10)\n","            image = cv2.putText(image.copy(), \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n","\n","    if image_original is None and keypoints_original is None:\n","        plt.figure(figsize=(40,40))\n","        plt.imshow(image)\n","\n","    else:\n","        for bbox in bboxes_original:\n","            start_point = (bbox[0], bbox[1])\n","            end_point = (bbox[2], bbox[3])\n","            image_original = cv2.rectangle(image_original.copy(), start_point, end_point, (0,255,0), 2)\n","        \n","        for kps in keypoints_original:\n","            for idx, kp in enumerate(kps):\n","                image_original = cv2.circle(image_original, tuple(kp), 5, (255,0,0), 10)\n","                image_original = cv2.putText(image_original, \" \" + keypoints_classes_ids2names[idx], tuple(kp), cv2.FONT_HERSHEY_SIMPLEX, 2, (255,0,0), 3, cv2.LINE_AA)\n","\n","        f, ax = plt.subplots(1, 2, figsize=(40, 20))\n","\n","        ax[0].imshow(image_original)\n","        ax[0].set_title('Original image', fontsize=fontsize)\n","\n","        ax[1].imshow(image)\n","        ax[1].set_title('Transformed image', fontsize=fontsize)\n","        \n","image = (batch[0][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n","bboxes = batch[1][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n","\n","keypoints = []\n","for kps in batch[1][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n","    keypoints.append([kp[:2] for kp in kps])\n","\n","image_original = (batch[2][0].permute(1,2,0).numpy() * 255).astype(np.uint8)\n","bboxes_original = batch[3][0]['boxes'].detach().cpu().numpy().astype(np.int32).tolist()\n","\n","keypoints_original = []\n","for kps in batch[3][0]['keypoints'].detach().cpu().numpy().astype(np.int32).tolist():\n","    keypoints_original.append([kp[:2] for kp in kps])\n","\n","visualize(image, bboxes, keypoints, image_original, bboxes_original, keypoints_original)"]},{"cell_type":"markdown","metadata":{"id":"pr5g53DTORoC"},"source":["# 5. Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6iLHmsClORoC"},"outputs":[],"source":["def get_model(num_keypoints, weights_path=None):\n","    \n","    anchor_generator = AnchorGenerator(sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.25, 0.5, 0.75, 1.0, 2.0, 3.0, 4.0))\n","    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=False,\n","                                                                   pretrained_backbone=True,\n","                                                                   num_keypoints=num_keypoints,\n","                                                                   num_classes = 2, # Background is the first class, object is the second class\n","                                                                   rpn_anchor_generator=anchor_generator)\n","\n","    if weights_path:\n","        state_dict = torch.load(weights_path)\n","        model.load_state_dict(state_dict)        \n","        \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlZ-vympORoC"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","KEYPOINTS_FOLDER_TRAIN = 'glue_tubes_keypoints_dataset_134imgs/train'\n","KEYPOINTS_FOLDER_TEST = 'glue_tubes_keypoints_dataset_134imgs/test'\n","\n","dataset_train = ClassDataset(KEYPOINTS_FOLDER_TRAIN, transform=train_transform(), demo=False)\n","dataset_test = ClassDataset(KEYPOINTS_FOLDER_TEST, transform=None, demo=False)\n","\n","data_loader_train = DataLoader(dataset_train, batch_size=3, shuffle=True, collate_fn=collate_fn)\n","data_loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n","\n","model = get_model(num_keypoints = 2)\n","model.to(device)\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n","num_epochs = 5\n","\n","for epoch in range(num_epochs):\n","    train_one_epoch(model, optimizer, data_loader_train, device, epoch, print_freq=1000)\n","    lr_scheduler.step()\n","    evaluate(model, data_loader_test, device)\n","    \n","# Save model weights after training\n","torch.save(model.state_dict(), 'keypointsrcnn_weights.pth')"]},{"cell_type":"markdown","metadata":{"id":"tfjyFyFXORoC"},"source":["# 6. Visualizing model predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fd3D7tRORoD"},"outputs":[],"source":["iterator = iter(data_loader_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kADC8NI-ORoD"},"outputs":[],"source":["images, targets = next(iterator)\n","images = list(image.to(device) for image in images)\n","\n","with torch.no_grad():\n","    model.to(device)\n","    model.eval()\n","    output = model(images)\n","\n","print(\"Predictions: \\n\", output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvA_o9nXORoD"},"outputs":[],"source":["image = (images[0].permute(1,2,0).detach().cpu().numpy() * 255).astype(np.uint8)\n","scores = output[0]['scores'].detach().cpu().numpy()\n","\n","high_scores_idxs = np.where(scores > 0.7)[0].tolist() # Indexes of boxes with scores > 0.7\n","post_nms_idxs = torchvision.ops.nms(output[0]['boxes'][high_scores_idxs], output[0]['scores'][high_scores_idxs], 0.3).cpu().numpy() # Indexes of boxes left after applying NMS (iou_threshold=0.3)\n","\n","# Below, in output[0]['keypoints'][high_scores_idxs][post_nms_idxs] and output[0]['boxes'][high_scores_idxs][post_nms_idxs]\n","# Firstly, we choose only those objects, which have score above predefined threshold. This is done with choosing elements with [high_scores_idxs] indexes\n","# Secondly, we choose only those objects, which are left after NMS is applied. This is done with choosing elements with [post_nms_idxs] indexes\n","\n","keypoints = []\n","for kps in output[0]['keypoints'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n","    keypoints.append([list(map(int, kp[:2])) for kp in kps])\n","\n","bboxes = []\n","for bbox in output[0]['boxes'][high_scores_idxs][post_nms_idxs].detach().cpu().numpy():\n","    bboxes.append(list(map(int, bbox.tolist())))\n","    \n","visualize(image, bboxes, keypoints)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"KeypointRCNN_training.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}